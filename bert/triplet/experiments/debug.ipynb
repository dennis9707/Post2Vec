{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "def get_train_args():\n",
    "    args = easydict.EasyDict({\n",
    "            \"data_folder\": \"../../data/train\",\n",
    "            \"vocab_file\": \"../../data/tags/commonTags_post2vec.csv\",\n",
    "            \"logging_steps\": 500,\n",
    "            \"num_train_epochs\": 20,\n",
    "            \"per_gpu_train_batch_size\": 2,\n",
    "            \"seed\": 42,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"adam_epsilon\": 1e-8,\n",
    "            \"warmup_steps\":0,\n",
    "            \"code_bert\":'microsoft/codebert-base',\n",
    "            \"max_grad_norm\":1.0,\n",
    "            \"save_steps\": 1000,\n",
    "            \"output_dir\": \"../results\",\n",
    "            \"learning_rate\":1e-6,\n",
    "})\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"/usr/src/bert\")\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertConfig, get_linear_schedule_with_warmup, AutoTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gc\n",
    "from sklearn import metrics, preprocessing\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from util.util import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from model.loss import loss_fn\n",
    "from model.model import TBertT\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from data_structure.question import Question, QuestionDataset\n",
    "import pandas as pd\n",
    "from util.util import write_tensor_board\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_optimizer_scheduler(args, model, train_steps):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=train_steps\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def get_tag_encoder(vocab_file):\n",
    "    tab_vocab_path = vocab_file\n",
    "    tag_vocab = pd.read_csv(tab_vocab_path)\n",
    "    tag_list = tag_vocab[\"tag\"].astype(str).tolist()\n",
    "    mlb = preprocessing.MultiLabelBinarizer()\n",
    "    mlb.fit([tag_list])\n",
    "    return mlb, len(mlb.classes_)\n",
    "\n",
    "\n",
    "def init_train_env(args, tbert_type):\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"device: %s, n_gpu: %s\",\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "    )\n",
    "    \n",
    "    # Set seed\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    # get the encoder for tags\n",
    "    mlb, num_class = get_tag_encoder(args.vocab_file)\n",
    "    args.mlb = mlb\n",
    "    args.num_class = num_class\n",
    "    \n",
    "    # get the model\n",
    "    if tbert_type == 'trinity':\n",
    "        model = TBertT(BertConfig(), args.code_bert, args.num_class)\n",
    "    else:\n",
    "        raise Exception(\"TBERT type not found\")\n",
    "    \n",
    "    args.tbert_type = tbert_type\n",
    "    model.to(args.device)\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/22/2021 06:19:27 - WARNING - __main__ -   device: cuda, n_gpu: 8\n",
      "11/22/2021 06:19:45 - INFO - __main__ -   Training/evaluation parameters {'data_folder': '../../data/train', 'vocab_file': '../../data/tags/commonTags_post2vec.csv', 'logging_steps': 500, 'num_train_epochs': 20, 'per_gpu_train_batch_size': 2, 'seed': 42, 'gradient_accumulation_steps': 4, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'code_bert': 'microsoft/codebert-base', 'max_grad_norm': 1.0, 'save_steps': 1000, 'output_dir': '../results', 'learning_rate': 1e-06, 'n_gpu': 8, 'device': device(type='cuda'), 'mlb': MultiLabelBinarizer(classes=None, sparse_output=False), 'num_class': 23686, 'tbert_type': 'trinity'}\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5,6,7'\n",
    "args = get_train_args()\n",
    "model = init_train_env(args, tbert_type='trinity') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_paths_from_directory(input_dir):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file_name in files:\n",
    "            file_paths.append(os.path.join(root, file_name))\n",
    "    file_paths.sort()\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_paths_from_directory(args.data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total training examples 10279014\n",
    "train_numbers = 9765063\n",
    "# 每个epoch有几个batch\n",
    "epoch_batch_num = train_numbers / args.train_batch_size\n",
    "# 一共有几个step更新参数\n",
    "t_total = epoch_batch_num * args.num_train_epochs\n",
    "\n",
    "optimizer, scheduler = get_optimizer_scheduler(args, model, t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_dataset(mlb,file):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\", local_files_only=True)\n",
    "    train = pd.read_pickle(file)\n",
    "    training_set = QuestionDataset(train[:100], mlb, tokenizer)\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size):\n",
    "    data_loader = DataLoader(dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_cnt in range(len(files)):\n",
    "    tr_loss = 0\n",
    "    # Load dataset and dataloader\n",
    "    training_set = load_data_to_dataset(args.mlb, files[file_cnt])\n",
    "    train_size = int(0.95 * len(training_set))\n",
    "    valid_size = len(training_set) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(training_set, [train_size, valid_size])\n",
    "    args.train_batch_size = 2\n",
    "    train_data_loader = get_dataloader(train_dataset, args.train_batch_size)\n",
    "    valid_data_loader = get_dataloader(valid_dataset, args.train_batch_size)\n",
    "    break\n",
    "    # print(args.train_batch_size)\n",
    "    # print('############# FILE {}: Training Start   #############'.format(file_cnt))\n",
    "    # # Train!\n",
    "    # for epoch in range(args.num_train_epochs):\n",
    "    #     print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
    "    #     model.train()\n",
    "    #     for step, data in enumerate(train_data_loader):\n",
    "    #         title_ids = data['titile_ids'].to(args.device, dtype=torch.long)\n",
    "    #         title_mask = data['title_mask'].to(args.device, dtype=torch.long)\n",
    "    #         text_ids = data['text_ids'].to(args.device, dtype=torch.long)\n",
    "    #         text_mask = data['text_mask'].to(args.device, dtype=torch.long)\n",
    "    #         code_ids = data['code_ids'].to(args.device, dtype=torch.long)\n",
    "    #         code_mask = data['code_mask'].to(args.device, dtype=torch.long)\n",
    "    #         targets = data['labels'].to(args.device, dtype=torch.float)\n",
    "    #         model.zero_grad()\n",
    "    #         outputs = model(title_ids=title_ids,\n",
    "    #                         title_attention_mask=title_mask,\n",
    "    #                         text_ids=text_ids,\n",
    "    #                         text_attention_mask=text_mask,\n",
    "    #                         code_ids=code_ids,\n",
    "    #                         code_attention_mask=code_mask)\n",
    "\n",
    "    #         loss = loss_fn(outputs, targets)\n",
    "    #         loss.backward()\n",
    "    #         tr_loss += loss.item()\n",
    "\n",
    "            \n",
    "    #         torch.nn.utils.clip_grad_norm_(\n",
    "    #                 model.parameters(), args.max_grad_norm)\n",
    "    #         optimizer.step()\n",
    "    #         scheduler.step()            \n",
    "    #         if args.logging_steps > 0 and step % args.logging_steps == 0:\n",
    "    #             tb_data = {\n",
    "    #                 'lr': scheduler.get_last_lr()[0],\n",
    "    #                 'loss': tr_loss / args.logging_steps\n",
    "    #             }\n",
    "    #             print(f'tb_data, {tb_data}')\n",
    "    #             print(f'output size: {outputs.size()}')\n",
    "    #             print(f'target: target size: {targets.size()}')\n",
    "    #             print(\n",
    "    #                 f'Epoch: {epoch}, Batch: {step}， Loss:  {tr_loss / args.logging_steps}')\n",
    "    #             current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #             print(\"Current Time =\", current_time)\n",
    "    #             tr_loss = 0.0\n",
    "    #     # evaluation\n",
    "    #     print('############# Epoch {}: Training End     #############'.format(epoch))\n",
    "    #     print(\n",
    "    #         '############# Epoch {}: Validation Start   #############'.format(epoch))\n",
    "    #     model.eval()\n",
    "    #     fin_targets = []\n",
    "    #     fin_outputs = []\n",
    "    #     with torch.no_grad():\n",
    "    #         for batch_idx, data in enumerate(valid_data_loader, 0):\n",
    "    #             title_ids = data['titile_ids'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             title_mask = data['title_mask'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             text_ids = data['text_ids'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             text_mask = data['text_mask'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             code_ids = data['code_ids'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             code_mask = data['code_mask'].to(\n",
    "    #                 args.device, dtype=torch.long)\n",
    "    #             targets = data['labels'].to(\n",
    "    #                 args.device, dtype=torch.float)\n",
    "\n",
    "    #             outputs = model(title_ids=title_ids,\n",
    "    #                             title_attention_mask=title_mask,\n",
    "    #                             text_ids=text_ids,\n",
    "    #                             text_attention_mask=text_mask,\n",
    "    #                             code_ids=code_ids,\n",
    "    #                             code_attention_mask=code_mask)\n",
    "    #             # target = targets.cpu().detach().numpy().tolist()\n",
    "    #             # output = torch.sigmoid(\n",
    "    #             #     outputs).cpu().detach().numpy().tolist()\n",
    "    #             print(\"output\")\n",
    "    #             print(outputs)\n",
    "    #             print(outputs.size())\n",
    "    #             print(f'target: target size: {targets.size()}')\n",
    "    #             fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "    #             fin_outputs.extend(torch.sigmoid(\n",
    "    #                 outputs).cpu().detach().numpy().tolist())\n",
    "    #             [pre, rc, f1, cnt] = evaluate_batch(\n",
    "    #                 fin_outputs, fin_targets, [1, 2, 3, 4, 5])\n",
    "    #         print(f\"F1 Score = {pre}\")\n",
    "    #         print(f\"Recall Score  = {rc}\")\n",
    "    #         print(f\"Precision Score  = {f1}\")\n",
    "    #         print(f\"Count  = {cnt}\")\n",
    "    #         print(\n",
    "    #             '############# Epoch {}: Validation End     #############'.format(epoch))\n",
    "        \n",
    "    #     logger.info(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "F1 Score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Recall Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Precision Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Count  = 2\n",
      "4\n",
      "4\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "F1 Score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Recall Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Precision Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Count  = 4\n",
      "5\n",
      "5\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "0.0 0.0 0.0\n",
      "F1 Score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Recall Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Precision Score  = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Count  = 5\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():\n",
    "      fin_targets = []\n",
    "      fin_outputs = []\n",
    "      for batch_idx, data in enumerate(valid_data_loader, 0):\n",
    "            title_ids = data['titile_ids'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            title_mask = data['title_mask'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            text_ids = data['text_ids'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            text_mask = data['text_mask'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            code_ids = data['code_ids'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            code_mask = data['code_mask'].to(\n",
    "                  args.device, dtype=torch.long)\n",
    "            targets = data['labels'].to(\n",
    "                  args.device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(title_ids=title_ids,\n",
    "                              title_attention_mask=title_mask,\n",
    "                              text_ids=text_ids,\n",
    "                              text_attention_mask=text_mask,\n",
    "                              code_ids=code_ids,\n",
    "                              code_attention_mask=code_mask)\n",
    "            target = targets.cpu().detach().numpy().tolist()\n",
    "            output = torch.sigmoid(\n",
    "                  outputs).cpu().detach().numpy().tolist()\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(\n",
    "                  outputs).cpu().detach().numpy().tolist())\n",
    "            print(len(fin_targets))\n",
    "            print(len(fin_outputs))\n",
    "            [pre, rc, f1, cnt] = evaluate_batch(\n",
    "                  fin_outputs, fin_targets, [1, 2, 3, 4, 5])\n",
    "            print(f\"F1 Score = {pre}\")\n",
    "            print(f\"Recall Score  = {rc}\")\n",
    "            print(f\"Precision Score  = {f1}\")\n",
    "            print(f\"Count  = {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "3.0\n",
      "4.0\n",
      "4.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "for i in fin_targets:\n",
    "    print(sum(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fin_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ori(pred, label, topk):\n",
    "    \"\"\"\n",
    "    dimension of pred and label should be equal.\n",
    "    :param pred: a list of prediction\n",
    "    :param label: a list of true label\n",
    "    :param topk:\n",
    "    :return: a dictionary: {'precision': pre_k, 'recall': rec_k, 'f1': f1_k}\n",
    "    \"\"\"\n",
    "    top_idx_list = sorted(range(len(pred)), key=lambda i: pred[i])[-topk:]\n",
    "    num_of_true_in_topk = len([idx for idx in top_idx_list if label[idx] == 1])\n",
    "    # precision@k = #true label in topk / k\n",
    "    pre_k = num_of_true_in_topk / float(topk)\n",
    "    # recall@k = #true label in topk / #true label\n",
    "    num_of_true_in_all = sum(label)\n",
    "    if num_of_true_in_all > topk:\n",
    "        rec_k = num_of_true_in_topk / float(topk)\n",
    "    else:\n",
    "        rec_k = num_of_true_in_topk / float(num_of_true_in_all)\n",
    "    # f1@k = 2 * precision@k * recall@k / (precision@k + recall@k)\n",
    "    if pre_k == 0 and rec_k == 0:\n",
    "        f1_k = 0.0\n",
    "    else:\n",
    "        f1_k = 2 * pre_k * rec_k / (pre_k + rec_k)\n",
    "    # return {'precision': pre_k, 'recall': rec_k, 'f1': f1_k}\n",
    "    print(pre_k, rec_k, f1_k)\n",
    "    return pre_k, rec_k, f1_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch1(pred, label, topk_list=[1, 2, 3, 4, 5]):\n",
    "    pre = [0.0] * len(topk_list)\n",
    "    rc = [0.0] * len(topk_list)\n",
    "    f1 = [0.0] * len(topk_list)\n",
    "    cnt = 0\n",
    "    print(\"pre\")\n",
    "    print(pre)\n",
    "    for i in range(0, len(pred)):\n",
    "        for idx, topk in enumerate(topk_list):\n",
    "            pre_val, rc_val, f1_val = evaluate_ori(\n",
    "                pred=pred[i], label=label[i], topk=topk)\n",
    "            pre[idx] += pre_val\n",
    "            rc[idx] += rc_val\n",
    "            f1[idx] += f1_val\n",
    "        cnt += 1\n",
    "    print(\"result\")\n",
    "    print([pre, rc, f1, cnt])\n",
    "    pre[:] = [x / cnt for x in pre]\n",
    "    rc[:] = [x / cnt for x in rc]\n",
    "    f1[:] = [x / cnt for x in f1]\n",
    "    return [pre, rc, f1, cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "1.0 1.0 1.0\n",
      "0.5 0.5 0.5\n",
      "0.3333333333333333 0.5 0.4\n",
      "0.25 0.5 0.3333333333333333\n",
      "0.4 1.0 0.5714285714285715\n",
      "1.0 1.0 1.0\n",
      "1.0 1.0 1.0\n",
      "0.6666666666666666 0.6666666666666666 0.6666666666666666\n",
      "0.5 0.6666666666666666 0.5714285714285715\n",
      "0.6 1.0 0.7499999999999999\n",
      "result\n",
      "[[2.0, 1.5, 1.0, 0.75, 1.0], [2.0, 1.5, 1.1666666666666665, 1.1666666666666665, 2.0], [2.0, 1.5, 1.0666666666666667, 0.9047619047619049, 1.3214285714285714], 2]\n"
     ]
    }
   ],
   "source": [
    "[pre, rc, f1, cnt] = evaluate_batch1([[0,0,0,0,0],[0,0,0,0,1]], [[1,0,0,0,1],[1,0,0,1,1]], [1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.75, 0.5, 0.375, 0.5],\n",
       " [1.0, 0.75, 0.5833333333333333, 0.5833333333333333, 1.0],\n",
       " [1.0, 0.75, 0.5333333333333333, 0.45238095238095244, 0.6607142857142857],\n",
       " 2]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pre, rc, f1, cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
