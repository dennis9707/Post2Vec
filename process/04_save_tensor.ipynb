{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../bert\")\n",
    "sys.path.append(\"../..\")\n",
    "import multiprocessing as mp\n",
    "from typing import Counter\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from data_structure.question import Question, NewQuestion\n",
    "import pandas as pd\n",
    "from util.util import get_files_paths_from_directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/codebert-base\", local_files_only=True)\n",
    "def gen_feature(tokens, max_length):\n",
    "\n",
    "        feature = tokenizer(tokens, max_length=max_length,\n",
    "                                 padding='max_length', return_attention_mask=True,\n",
    "                                 return_token_type_ids=False, truncation=True,\n",
    "                                 return_tensors='pt')\n",
    "        res = {\n",
    "            \"input_ids\": feature[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": feature[\"attention_mask\"].flatten()}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"../data/processed_train\"\n",
    "out_dir = \"../data/tensor_data/\"\n",
    "files = get_files_paths_from_directory(input_dir)\n",
    "for file in files:\n",
    "    dataset = pd.read_pickle(file)\n",
    "    file_name = file[24:]\n",
    "    q_list = list()\n",
    "    for question in dataset:\n",
    "        qid = question.get_qid()\n",
    "        title = question.get_title()\n",
    "        title_feature = gen_feature(title, 100)\n",
    "        text = question.get_text()\n",
    "        text_feature = gen_feature(text, 512)\n",
    "        code = question.get_code()\n",
    "        code_feature = gen_feature(code, 512)\n",
    "        date = question.get_creation_date()\n",
    "        tags = question.get_tag\n",
    "        q_list.append(NewQuestion(qid, title, text, code, date, tags))\n",
    "    import pickle\n",
    "    with open(out_dir+file_name, 'wb') as f:\n",
    "        pickle.dump(q_list, f)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_feature['input_ids'].flatten().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am trying to plot a logistic regression graph from the following data however when i try: i get the following error: i am a bit confused why it thinks that either x or y has only one sample.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,   118,   524,   667,     7,  6197,    10,  7425,  5580, 39974,\n",
       "         20992,    31,     5,   511,   414,   959,    77,   939,   860,    35,\n",
       "           939,   120,     5,   511,  5849,    35,   939,   524,    10,   828,\n",
       "         10985,   596,    24,  4265,    14,  1169,  3023,    50,  1423,    34,\n",
       "           129,    65,  7728,     4,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) clf = linear_model.logisticregression (c=1e5) clf.fit (x, y) valueerror: found input variables with inconsistent numbers of samples: [1 , 12 ]\n",
    "x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) clf = linear_model.logisticregression (c=1e5) clf.fit (x, y) valueerror: found input variables with inconsistent numbers of samples: [1 , 12 ]\n",
    "x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) clf = linear_model.logisticregression (c=1e5) clf.fit (x, y) valueerror: found input variables with inconsistent numbers of samples: [1 , 12 ]\n",
    "x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) clf = linear_model.logisticregression (c=1e5) clf.fit (x, y) valueerror: found input variables with inconsistent numbers of samples: [1 , 12 ]\n",
    "x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model x = np.array ([ 0,1,2,3,4,5,6,7,8,9,10,11] ) y = np.array ([ 0,0,0,0,1,0,1,0,1,1,1,1] ) clf = linear_model.logisticregression (c=1e5) clf.fit (x, y) valueerror: found input variables with inconsistent numbers of samples: [1 , 12 ]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
